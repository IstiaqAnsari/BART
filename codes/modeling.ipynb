{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a3b9d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "bpe = BPEmb(lang='bn',vs=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "d096ef7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1524, 3197, 834, 3918, 1849, 2]"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode_ids_with_eos(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "de806478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'আজ আমার মন ভালো নেই'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode_ids([1, 1524, 3197, 834, 3918, 1849, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "70a144d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[207]"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode_ids('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "72ce694b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1524, 3197, 834, 3918, 1849]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"আজ আমার মন ভালো নেই\"\n",
    "bpe.encode_ids(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e564a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ansari/codes/spellchecker/bart/codes'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4faf07",
   "metadata": {},
   "source": [
    "# Finding the config of bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "0a19a669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': 'facebook/bart-large-cnn',\n",
       " '_num_labels': 3,\n",
       " 'activation_dropout': 0.0,\n",
       " 'activation_function': 'gelu',\n",
       " 'add_final_layer_norm': False,\n",
       " 'architectures': ['BartForConditionalGeneration'],\n",
       " 'attention_dropout': 0.0,\n",
       " 'bos_token_id': 0,\n",
       " 'classif_dropout': 0.0,\n",
       " 'classifier_dropout': 0.0,\n",
       " 'd_model': 1024,\n",
       " 'decoder_attention_heads': 16,\n",
       " 'decoder_ffn_dim': 4096,\n",
       " 'decoder_layerdrop': 0.0,\n",
       " 'decoder_layers': 12,\n",
       " 'decoder_start_token_id': 2,\n",
       " 'dropout': 0.1,\n",
       " 'early_stopping': True,\n",
       " 'encoder_attention_heads': 16,\n",
       " 'encoder_ffn_dim': 4096,\n",
       " 'encoder_layerdrop': 0.0,\n",
       " 'encoder_layers': 12,\n",
       " 'eos_token_id': 2,\n",
       " 'force_bos_token_to_be_generated': True,\n",
       " 'forced_bos_token_id': 0,\n",
       " 'forced_eos_token_id': 2,\n",
       " 'gradient_checkpointing': False,\n",
       " 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1', '2': 'LABEL_2'},\n",
       " 'init_std': 0.02,\n",
       " 'is_encoder_decoder': True,\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n",
       " 'length_penalty': 2.0,\n",
       " 'max_length': 142,\n",
       " 'max_position_embeddings': 1024,\n",
       " 'min_length': 56,\n",
       " 'model_type': 'bart',\n",
       " 'no_repeat_ngram_size': 3,\n",
       " 'normalize_before': False,\n",
       " 'num_beams': 4,\n",
       " 'num_hidden_layers': 12,\n",
       " 'output_past': True,\n",
       " 'pad_token_id': 1,\n",
       " 'prefix': ' ',\n",
       " 'scale_embedding': False,\n",
       " 'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "   'length_penalty': 2.0,\n",
       "   'max_length': 142,\n",
       "   'min_length': 56,\n",
       "   'no_repeat_ngram_size': 3,\n",
       "   'num_beams': 4}},\n",
       " 'transformers_version': '4.2.2',\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 50264}"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"config.json\") as f:\n",
    "    a = json.load(f)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "b2bab5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = BartConfig(a)\n",
    "# config = model.config\n",
    "# model.config.save_pretrained(\"bart.config\")\n",
    "config = BartConfig.from_pretrained(\"bart.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "286b541c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "aaa76ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model \n",
    "model = BartForConditionalGeneration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48e39d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartModel, BartForConditionalGeneration, BartConfig,BartTokenizer\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60099d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63957edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 100, 524, 2051, 16866, 1512, 2], [0, 13755, 47, 8578, 17487, 2, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0]]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.prepare_seq2seq_batch(['I am fine Ansari', 'Are you okay ?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9608baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f27c2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'_name_or_path': 'facebook/bart-large-cnn', '_num_labels': 3, 'activation_dropout': 0.0, 'activation_function': 'gelu', 'add_final_layer_norm': False, 'architectures': ['BartForConditionalGeneration'], 'attention_dropout': 0.0, 'bos_token_id': 0, 'classif_dropout': 0.0, 'classifier_dropout': 0.0, 'd_model': 1024, 'decoder_attention_heads': 16, 'decoder_ffn_dim': 4096, 'decoder_layerdrop': 0.0, 'decoder_layers': 12, 'decoder_start_token_id': 2, 'dropout': 0.1, 'early_stopping': True, 'encoder_attention_heads': 16, 'encoder_ffn_dim': 4096, 'encoder_layerdrop': 0.0, 'encoder_layers': 12, 'eos_token_id': 2, 'force_bos_token_to_be_generated': True, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2, 'gradient_checkpointing': False, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1', '2': 'LABEL_2'}, 'init_std': 0.02, 'is_encoder_decoder': True, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}, 'length_penalty': 2.0, 'max_length': 142, 'max_position_embeddings': 1024, 'min_length': 56, 'model_type': 'bart', 'no_repeat_ngram_size': 3, 'normalize_before': False, 'num_beams': 4, 'num_hidden_layers': 12, 'output_past': True, 'pad_token_id': 1, 'prefix': ' ', 'scale_embedding': False, 'task_specific_params': {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}}, 'transformers_version': '4.2.2', 'use_cache': True, 'vocab_size': 50264}\n",
    "config = BartConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67005ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/transformers/__init__.py'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# model = BartForConditionalGeneration(config)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "95912fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><pad></s><unk>'"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "6d15927e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  8332,   947,   717,  2305,    24,  1768,     5,   909,  4518,\n",
       "            11,  1263,     7,  5876,    13,   239,  2372,  2876,  3841,  1274,\n",
       "             4,    20,  4374,    16,     7,  1888,     5,   810,     9, 12584,\n",
       "             4,  9221,  5735,  7673,   916,    58,  1768,     7,    28,  2132,\n",
       "            30,     5,  2572, 10816,    61,    58,   421,     7,    94,   149,\n",
       "            23,   513, 15372,  3859,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "34609f79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BartTokenizer' object has no attribute 'call'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_152702/932986216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Today I am sad.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BartTokenizer' object has no attribute 'call'"
     ]
    }
   ],
   "source": [
    "sent = \"Today I am sad.\"\n",
    "tokenizer.encode(sent), tokenizer.call(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = tokenizer.batch_encode_plus(example_batch['text'], pad_to_max_length=True, max_length=1024, truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "288f1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IDS = torch.cat((inputs[\"input_ids\"],inputs[\"input_ids\"]), axis=0)\n",
    "ATTN_MASK = torch.cat((inputs[\"attention_mask\"],inputs[\"attention_mask\"]), axis=0)\n",
    "LABELS = torch.cat((labels,labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "743793fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c77deef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    0, 8332,  947,  717, 1768,    5,  909, 4518,   11, 1263,    7,\n",
       "         5876,   13,  239, 2372, 2876, 3841, 1274,    2]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34c7275d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Slam Slam Slam geared geared Vox Voxgren Slam Slam recurrent recurrent geared geared geared Maver Maver'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "989100a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8940, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model(input_ids = inputs[\"input_ids\"],attention_mask = inputs[\"attention_mask\"], labels = inputs[\"input_ids\"])\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "34c24333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  8332,   947,   717,  2305,    24,  1768,     5,   909,  4518,\n",
       "            11,  1263,     7,  5876,    13,   239,  2372,  2876,  3841,  1274,\n",
       "             4,    20,  4374,    16,     7,  1888,     5,   810,     9, 12584,\n",
       "             4,  9221,  5735,  7673,   916,    58,  1768,     7,    28,  2132,\n",
       "            30,     5,  2572, 10816,    61,    58,   421,     7,    94,   149,\n",
       "            23,   513, 15372,  3859,     4,     2]])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "4a731eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labuls = torch.tensor([[1]*56]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "4b9c8162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8332,  8332,   947,   717,  1768,    24,  1768,     5,   909,  4518,\n",
       "            11,  1263,     7,  5876,    13,   239,  2372,  2876,  3841,  1274,\n",
       "             4,  9221,  4374,    16,     7,  1888,     5,   810,     9, 12584,\n",
       "             4,  9221,  5735,  7673,   916,    58,  1768,     7,    28,  2132,\n",
       "            30,     5,  2572, 10816,     4,    58,   421,     7,    94,   149,\n",
       "            23,   513, 15372,  3859,     4,     2]])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.argmax(out.logits, axis=2)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "7565da24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8332,  811,   20, 5735, 3913, 1768,  916, 2572, 4518,   32,   20,    7,\n",
       "         5876, 2372,   20, 2372,    4,   20, 1274,    4]])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "9866de93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[11.8544],\n",
       "          [11.0273],\n",
       "          [10.9150],\n",
       "          [13.9413],\n",
       "          [10.4414],\n",
       "          [11.5450],\n",
       "          [ 8.8487],\n",
       "          [12.5335],\n",
       "          [11.4360],\n",
       "          [11.8563],\n",
       "          [11.5445],\n",
       "          [12.6755],\n",
       "          [11.8272],\n",
       "          [14.3440],\n",
       "          [10.5630],\n",
       "          [14.0671],\n",
       "          [12.2241],\n",
       "          [11.3962],\n",
       "          [13.3064],\n",
       "          [12.6422]]], grad_fn=<TopkBackward>),\n",
       " tensor([[[8332],\n",
       "          [ 811],\n",
       "          [  20],\n",
       "          [5735],\n",
       "          [3913],\n",
       "          [1768],\n",
       "          [ 916],\n",
       "          [2572],\n",
       "          [4518],\n",
       "          [  32],\n",
       "          [  20],\n",
       "          [   7],\n",
       "          [5876],\n",
       "          [2372],\n",
       "          [  20],\n",
       "          [2372],\n",
       "          [   4],\n",
       "          [  20],\n",
       "          [1274],\n",
       "          [   4]]]))"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v,idx = torch.topk((out.logits), k=1,axis=2)\n",
    "v,idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "1d93cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = idx.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667e996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c3bbd4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 0.7000, 0.1000, 0.1000]]), tensor([1]), tensor(0.9732))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpuT = torch.tensor([[.1,.7,.1,.1]]).float()\n",
    "target = torch.tensor([1]).long()\n",
    "output = loss_fct(inpuT, target)\n",
    "inpuT, target, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "477a740c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152702/2635051294.py:2: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_softmax(torch.tensor([[.1,.7,.1,.1]]).float())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5732, -0.9732, -1.5732, -1.5732]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "log_softmax(torch.tensor([[.1,.7,.1,.1]]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a030c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([[    0,  8332,   947,   717,  2305,    24,  1768,     5,   909,  4518,\n",
    "            11,  1263,     7,  5876,    13,   239,  2372,  2876,  3841,  1274,\n",
    "             4,    20,  4374,    16,     7,  1888,     5,   810,     9, 12584,\n",
    "             4,  9221,  5735,  7673,   916,    58,  1768,     7,    28,  2132,\n",
    "            30,     5,  2572, 10816,    61,    58,   421,     7,    94,   149,\n",
    "            23,   513, 15372,  3859,     4,     2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(article_input_ids,num_beams=4,length_penalty=2.0,max_length=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85370165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24fe92c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_192227/3712685849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m summary_txt = tokenizer.decode(torch.tensor([[    2,     0, 12133, 23334, 23334, 23334, 29247, 29247, 29247, 12113,\n\u001b[0m\u001b[1;32m      2\u001b[0m          12113, 12113, 24235, 24235, 47927, 47927, 47927, 24235, 24235,     2]]), skip_special_tokens=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "summary_txt = tokenizer.decode(torch.tensor([[    2,     0, 12133, 23334, 23334, 23334, 29247, 29247, 29247, 12113,\n",
    "         12113, 12113, 24235, 24235, 47927, 47927, 47927, 24235, 24235,     2]]), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34e1e7",
   "metadata": {},
   "source": [
    "# Mask filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "49175dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7c4e8953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation_utils:\n",
      "\n",
      "generate(input_ids: Union[torch.LongTensor, NoneType] = None, max_length: Union[int, NoneType] = None, min_length: Union[int, NoneType] = None, do_sample: Union[bool, NoneType] = None, early_stopping: Union[bool, NoneType] = None, num_beams: Union[int, NoneType] = None, temperature: Union[float, NoneType] = None, top_k: Union[int, NoneType] = None, top_p: Union[float, NoneType] = None, repetition_penalty: Union[float, NoneType] = None, bad_words_ids: Union[Iterable[int], NoneType] = None, bos_token_id: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, length_penalty: Union[float, NoneType] = None, no_repeat_ngram_size: Union[int, NoneType] = None, num_return_sequences: Union[int, NoneType] = None, decoder_start_token_id: Union[int, NoneType] = None, use_cache: Union[bool, NoneType] = None, num_beam_groups: Union[int, NoneType] = None, diversity_penalty: Union[float, NoneType] = None, prefix_allowed_tokens_fn: Union[Callable[[int, torch.Tensor], List[int]], NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor] method of transformers.models.bart.modeling_bart.BartForConditionalGeneration instance\n",
      "    Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
      "    multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
      "    \n",
      "    Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
      "    attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
      "    indicated are the default values of those config.\n",
      "    \n",
      "    Most of these parameters are explained in more detail in `this blog post\n",
      "    <https://huggingface.co/blog/how-to-generate>`__.\n",
      "    \n",
      "    Parameters:\n",
      "    \n",
      "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      "            The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
      "            :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
      "        max_length (:obj:`int`, `optional`, defaults to 20):\n",
      "            The maximum length of the sequence to be generated.\n",
      "        min_length (:obj:`int`, `optional`, defaults to 10):\n",
      "            The minimum length of the sequence to be generated.\n",
      "        do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to use sampling ; use greedy decoding otherwise.\n",
      "        early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
      "        num_beams (:obj:`int`, `optional`, defaults to 1):\n",
      "            Number of beams for beam search. 1 means no beam search.\n",
      "        temperature (:obj:`float`, `optional`, defaults tp 1.0):\n",
      "            The value used to module the next token probabilities.\n",
      "        top_k (:obj:`int`, `optional`, defaults to 50):\n",
      "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "        top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
      "            higher are kept for generation.\n",
      "        repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
      "            <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
      "        pad_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `padding` token.\n",
      "        bos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `beginning-of-sequence` token.\n",
      "        eos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `end-of-sequence` token.\n",
      "        length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
      "            model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
      "            sequences.\n",
      "        no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
      "            If set to int > 0, all ngrams of that size can only occur once.\n",
      "        bad_words_ids(:obj:`List[List[int]]`, `optional`):\n",
      "            List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
      "            should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
      "            add_prefix_space=True).input_ids`.\n",
      "        num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
      "            The number of independently computed returned sequences for each element in the batch.\n",
      "        attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
      "            tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same\n",
      "            shape as :obj:`input_ids` that masks the pad token. `What are attention masks?\n",
      "            <../glossary.html#attention-mask>`__\n",
      "        decoder_start_token_id (:obj:`int`, `optional`):\n",
      "            If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
      "        use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      "            speed up decoding.\n",
      "        num_beam_groups (:obj:`int`, `optional`, defaults to 1):\n",
      "            Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
      "            beams. `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
      "        diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):\n",
      "            This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
      "            at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is\n",
      "            enabled.\n",
      "        prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments :obj:`inputs_ids` and the batch ID\n",
      "            :obj:`batch_id`. It has to return a list with the allowed tokens for the next generation step\n",
      "            conditioned on the previously generated tokens :obj:`inputs_ids` and the batch ID :obj:`batch_id`. This\n",
      "            argument is useful for constrained generation conditioned on the prefix, as described in\n",
      "            `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__.\n",
      "        output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      "            returned tensors for more details.\n",
      "        output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      "            for more details.\n",
      "        output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      "        return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      "    \n",
      "        model_kwargs:\n",
      "            Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the\n",
      "            model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific\n",
      "            kwargs should be prefixed with `decoder_`.\n",
      "    \n",
      "    Return:\n",
      "        :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A\n",
      "        :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
      "        ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.\n",
      "    \n",
      "            If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
      "            possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
      "    \n",
      "                - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`\n",
      "    \n",
      "            If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
      "            :class:`~transformers.file_utils.ModelOutput` types are:\n",
      "    \n",
      "                - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`\n",
      "    \n",
      "    Examples::\n",
      "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      "        >>> # do greedy decoding without providing a prompt\n",
      "        >>> outputs = model.generate(max_length=40)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      "        >>> document = (\n",
      "        ... \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
      "        ... \"in the strife-torn southern philippines on monday , the military said.\"\n",
      "        ... )\n",
      "        >>> # encode input contex\n",
      "        >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
      "        >>> # with T5 encoder-decoder model conditioned on short news article.\n",
      "        >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
      "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      "        >>> input_context = \"The dog\"\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate 3 candidates using sampling\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
      "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
      "        >>> # \"Legal\" is one of the control codes for ctrl\n",
      "        >>> input_context = \"Legal My neighbor is\"\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "        >>> input_context = \"My cute dog\"\n",
      "        >>> # get tokens of words that should not be generated\n",
      "        >>> bad_words_ids = [tokenizer(bad_word, add_prefix_space=True).input_ids for bad_word in [\"idiot\", \"stupid\", \"shut up\"]]\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate sequences without allowing bad_words to be generated\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f6585e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_152702/1667332453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 )\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1172\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1046\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/home/sabbir_j/anaconda3/envs/bnlp_env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "out = model(input_ids).logits\n",
    "logits = out.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "\n",
    "tokenizer.decode(predictions).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc9d7e",
   "metadata": {},
   "source": [
    "# Decoder input ids and attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "cc2ca312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right, _make_causal_mask, _expand_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "3c1659fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1,4,5,6,7,2], [1,4,5,2,0,0]])\n",
    "output_ids = torch.tensor([[4,4,5,5,6,6,7,7,2], [4,4,5,5,2,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "2b85796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function shift_tokens_right in module transformers.models.bart.modeling_bart:\n",
      "\n",
      "shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int)\n",
      "    Shift input ids one token to the right.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(shift_tokens_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "7dfb2bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = _make_causal_mask(output_ids.size(), input_ids.dtype)\n",
    "cmm = cm.clone()\n",
    "cmm[cm == -9223372036854775808] = 1\n",
    "cmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174e517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "4039a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids!=0).long()\n",
    "out_attention_mask = (output_ids!=0).long()\n",
    "attention_mask,out_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "baed8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=input_ids, attention_mask = attention_mask, decoder_input_ids = output_ids, decoder_attention_mask = out_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "dd0041bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 200000])"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "7182510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "bb1b5a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2295a",
   "metadata": {},
   "source": [
    "# Loss niye khela kori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8505120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fct = CrossEntropyLoss()\n",
    "softmax = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9cbc4966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6ea90fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152702/1720163948.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output2 = loss_fct(softmax(inpuT), target)\n",
      "/tmp/ipykernel_152702/1720163948.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  inpuT, softmax(inpuT),target, output,output2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[11.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [11.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [11.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         ...,\n",
       "         [11.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [11.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "         [11.,  1.,  1.,  ...,  1.,  1.,  1.]]),\n",
       " tensor([[3.0469e-01, 1.3833e-05, 1.3833e-05,  ..., 1.3833e-05, 1.3833e-05,\n",
       "          1.3833e-05],\n",
       "         [3.0469e-01, 1.3833e-05, 1.3833e-05,  ..., 1.3833e-05, 1.3833e-05,\n",
       "          1.3833e-05],\n",
       "         [3.0469e-01, 1.3833e-05, 1.3833e-05,  ..., 1.3833e-05, 1.3833e-05,\n",
       "          1.3833e-05],\n",
       "         ...,\n",
       "         [3.0469e-01, 1.3833e-05, 1.3833e-05,  ..., 1.3833e-05, 1.3833e-05,\n",
       "          1.3833e-05],\n",
       "         [3.0469e-01, 1.3833e-05, 1.3833e-05,  ..., 1.3833e-05, 1.3833e-05,\n",
       "          1.3833e-05],\n",
       "         [3.0469e-01, 1.3833e-05, 1.3833e-05,  ..., 1.3833e-05, 1.3833e-05,\n",
       "          1.3833e-05]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(1.1885),\n",
       " tensor(10.5203))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpuT = torch.tensor([[11]+[1]*50263]*20).float()\n",
    "target = torch.tensor([0]*20).long()\n",
    "output = loss_fct((inpuT), target)\n",
    "output2 = loss_fct(softmax(inpuT), target)\n",
    "inpuT, softmax(inpuT),target, output,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2eb55508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152702/1181773492.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax(inpuT)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(inpuT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a273d01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(inpuT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "13be0104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function math.log>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "83dc01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000050000287824e-05"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 1\n",
    "a = 0.99999\n",
    "(y*math.log(a)+(1-y)*math.log(1-a))*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b268c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds = torch.mean(out.logits,axis=2),torch.std(out.logits,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "4545b677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "80212b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = torch.stack([torch.normal(x.item(),y.item(),size=(1,model.config.vocab_size)) for x,y in zip(means[0,:], stds[0])])\n",
    "logits2[:,0,0] = torch.tensor([11]*len(stds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "700a54cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fae98c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = logits2.permute(1,0,2)\n",
    "logits2 = logits2[0,:,:]\n",
    "target = torch.tensor([0]*len(stds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3218ad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 50264])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "7377cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152702/422661247.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output2 = loss_fct(softmax(logits2), target)\n",
      "/tmp/ipykernel_152702/422661247.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits2, softmax(logits2),target, output,output2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1000e+01, -4.1907e-01,  1.7334e+00,  ..., -7.8270e-01,\n",
       "           2.1868e+00,  2.6956e-02],\n",
       "         [ 1.1000e+01,  3.0929e+00,  8.8143e-01,  ..., -9.4053e-01,\n",
       "           4.9956e-03,  2.4194e+00],\n",
       "         [ 1.1000e+01,  2.1822e-01,  2.2500e-01,  ..., -2.1329e+00,\n",
       "           3.8443e-01, -1.4089e+00],\n",
       "         ...,\n",
       "         [ 1.1000e+01, -2.7124e+00, -4.0935e+00,  ...,  1.0854e+00,\n",
       "          -6.6496e+00,  1.0127e+00],\n",
       "         [ 1.1000e+01,  2.1054e-01, -2.6392e+00,  ...,  7.0419e-02,\n",
       "          -1.0109e+00, -2.6814e+00],\n",
       "         [ 1.1000e+01,  1.2023e+00,  4.3087e-01,  ...,  4.2815e-02,\n",
       "          -3.3244e-01, -3.8566e-01]]),\n",
       " tensor([[2.6268e-01, 2.8853e-06, 2.4830e-05,  ..., 2.0057e-06, 3.9077e-05,\n",
       "          4.5071e-06],\n",
       "         [1.8109e-01, 6.6662e-05, 7.3020e-06,  ..., 1.1808e-06, 3.0396e-06,\n",
       "          3.3993e-05],\n",
       "         [3.5379e-01, 7.3498e-06, 7.3999e-06,  ..., 7.0020e-07, 8.6788e-06,\n",
       "          1.4443e-06],\n",
       "         ...,\n",
       "         [4.1967e-01, 4.6524e-07, 1.1692e-07,  ..., 2.0752e-05, 9.0738e-09,\n",
       "          1.9296e-05],\n",
       "         [3.8630e-01, 7.9638e-06, 4.6077e-07,  ..., 6.9226e-06, 2.3478e-06,\n",
       "          4.4174e-07],\n",
       "         [5.2287e-01, 2.9061e-05, 1.3436e-05,  ..., 9.1149e-06, 6.2630e-06,\n",
       "          5.9384e-06]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(1.0172),\n",
       " tensor(10.4547))"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = loss_fct((logits2), target)\n",
    "output2 = loss_fct(softmax(logits2), target)\n",
    "logits2, softmax(logits2),target, output,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b48c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
